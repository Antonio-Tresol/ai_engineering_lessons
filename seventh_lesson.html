<!DOCTYPE html>
<html lang="en" class="dark scroll-smooth">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 7: Finetuning</title>

    <!-- Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;700&family=Source+Code+Pro:wght@400;500&display=swap" rel="stylesheet">

    <!-- KaTeX for Mathematical Formula Rendering -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" crossorigin="anonymous"
        onload="renderMathInElement(document.body, { delimiters: [ {left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false} ] });"></script>
    
    <!-- Local Tailwind Build -->
    <link rel="stylesheet" href="/css/main.min.css">

    <style>
        /* Navigation active state */
        .nav-link {
            transition: color 0.3s;
        }
        .nav-link.active {
            color: #6366f1;
            font-weight: 600;
        }
        
        code {
            font-family: 'Source Code Pro', monospace;
            background-color: #111827;
            padding: 0.125rem 0.375rem;
            border-radius: 0.25rem;
            font-size: 0.875rem;
            color: #10b981;
        }
        
        .katex {
            font-size: 1.1em;
        }
    </style>
</head>
<body class="bg-gray-900 text-gray-200 font-sans">

    <!-- Sticky Navigation -->
    <nav class="sticky top-0 z-50 bg-gray-900/80 backdrop-blur-lg border-b border-gray-800">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex items-center justify-between h-16">
                <a href="index.html" class="text-xl font-bold text-white hover:text-blue-400 transition-colors">AI Engineering</a>
                <div class="flex items-center space-x-4 md:space-x-8 text-sm md:text-base">
                    <a href="#overview" class="nav-link text-gray-400 hover:text-white">Overview</a>
                    <a href="#foundations" class="nav-link text-gray-400 hover:text-white">Foundations</a>
                    <a href="#memory" class="nav-link text-gray-400 hover:text-white">Memory</a>
                    <a href="#techniques" class="nav-link text-gray-400 hover:text-white">Techniques</a>
                    <a href="#practical" class="nav-link text-gray-400 hover:text-white">Practical</a>
                </div>
            </div>
        </div>
    </nav>

    <!-- Header -->
    <header class="text-center py-20 bg-gray-900">
        <h1 class="text-4xl md:text-5xl font-bold text-white tracking-tight">Chapter 7: Finetuning</h1>
        <p class="mt-4 text-lg text-gray-400">Advanced Model Adaptation for Specific Tasks</p>
        <p class="text-gray-400">AI Engineering by Huyen (2025)</p>
    </header>

    <main>
        <!-- Section 1: Overview -->
        <section id="overview" class="min-h-screen flex flex-col justify-center items-center px-8 py-24 border-b border-gray-800">
            <div class="max-w-5xl w-full">
                <h2 class="text-3xl font-bold text-white mb-12 text-center animated">Finetuning Overview</h2>
                
                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 mb-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-4">What is Finetuning?</h3>
                    <p class="text-gray-300 mb-4">Finetuning is the process of adapting a model to a specific task by further training the whole model or part of the model by adjusting its weights. Unlike prompt-based methods which adapt a model through instructions, context, and tools, finetuning modifies the model's parameters directly.</p>
                    <blockquote class="border-l-4 border-blue-500 pl-6 italic text-gray-400 my-6">
                        <p>"Finetuning is the process of adapting a model to a specific task by further training the whole model or part of the model... by adjusting its weights."</p>
                        <footer class="text-right mt-2 text-gray-500">- Huyen (2025, p. 307)</footer>
                    </blockquote>
                </div>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 mb-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-4">Goals of Finetuning</h3>
                    <ul class="list-disc list-inside space-y-3 text-gray-300">
                        <li><strong class="text-white">Improve domain-specific capabilities:</strong> Enhance performance on specialized tasks or knowledge domains</li>
                        <li><strong class="text-white">Strengthen safety:</strong> Reduce harmful or inappropriate outputs</li>
                        <li><strong class="text-white">Improve instruction-following:</strong> Better align model behavior with user intent</li>
                        <li><strong class="text-white">Increase sample efficiency:</strong> Learn the same behavior with fewer examples through transfer learning</li>
                    </ul>
                </div>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-4">Types of Finetuning</h3>
                    <div class="grid md:grid-cols-3 gap-6">
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-xl mb-3 text-blue-400">Supervised Finetuning</h4>
                            <p class="text-gray-300">Uses labeled input-output pairs to teach specific behaviors</p>
                        </div>
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-xl mb-3 text-purple-400">Preference Finetuning</h4>
                            <p class="text-gray-300">Uses comparative data to align model outputs with human preferences</p>
                        </div>
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-xl mb-3 text-green-400">Continued Pre-training</h4>
                            <p class="text-gray-300">Uses domain-specific raw text to expand model knowledge</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section 2: Foundations -->
        <section id="foundations" class="min-h-screen flex flex-col justify-center items-center px-8 py-24 border-b border-gray-800">
            <div class="max-w-5xl w-full">
                <h2 class="text-3xl font-bold text-white mb-12 text-center animated">Foundations of Finetuning</h2>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 mb-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-4">When to Finetune</h3>
                    <p class="text-gray-300 mb-6">Finetuning requires significantly more resources in data, hardware, and expertise. It should be attempted only after extensive experimentation with prompt-based methods.</p>
                    
                    <div class="grid md:grid-cols-2 gap-6">
                        <div class="bg-green-900/20 border border-green-700 p-6 rounded-lg">
                            <h4 class="font-bold text-xl mb-3 text-green-400">Reasons TO Finetune</h4>
                            <ul class="list-disc list-inside space-y-2 text-gray-300">
                                <li>Improving quality for specific structures (JSON, YAML, SQL)</li>
                                <li>Learning customer-specific patterns or dialects</li>
                                <li>Reducing latency by using smaller models</li>
                                <li>Lowering costs with more efficient models</li>
                                <li>Improving privacy and data security</li>
                                <li>Increasing control and ownership</li>
                            </ul>
                        </div>
                        <div class="bg-red-900/20 border border-red-700 p-6 rounded-lg">
                            <h4 class="font-bold text-xl mb-3 text-red-400">When NOT to Finetune</h4>
                            <ul class="list-disc list-inside space-y-2 text-gray-300">
                                <li>Adding new knowledge (use RAG instead)</li>
                                <li>Limited training data available</li>
                                <li>Prompt engineering hasn't been exhausted</li>
                                <li>Insufficient resources or expertise</li>
                                <li>Rapid iteration required</li>
                            </ul>
                        </div>
                    </div>

                    <blockquote class="border-l-4 border-blue-500 pl-6 italic text-gray-400 my-6">
                        <p>"OpenAI's InstructGPT paper suggested viewing finetuning as unlocking the capabilities a model already has but that are difficult for users to access via prompting alone."</p>
                        <footer class="text-right mt-2 text-gray-500">- Huyen (2025, p. 309)</footer>
                    </blockquote>
                </div>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-4">Transfer Learning Foundation</h3>
                    <p class="text-gray-300 mb-4">Finetuning is a practical application of transfer learning, which enables models to leverage knowledge from pre-training to learn new tasks more efficiently.</p>
                    <div class="bg-indigo-900/20 border border-indigo-700 p-6 rounded-lg">
                        <p class="text-gray-300"><strong class="text-indigo-400">Key Benefit:</strong> Transfer learning improves sample efficiency, allowing a model to learn the same behavior with fewer examples compared to training from scratch.</p>
                        <p class="text-gray-400 mt-2 text-sm">Reference: Huyen (2025, p. 309)</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section 3: Memory Bottlenecks -->
        <section id="memory" class="min-h-screen flex flex-col justify-center items-center px-8 py-24 border-b border-gray-800">
            <div class="max-w-5xl w-full">
                <h2 class="text-3xl font-bold text-white mb-12 text-center animated">The Core Challenge: Memory Bottlenecks</h2>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 mb-8 animated">
                    <h3 class="text-2xl font-semibold text-red-400 mb-4">Why Memory Matters</h3>
                    <p class="text-gray-300 mb-6">The primary technical challenge in finetuning is memory consumption. Training requires significantly more memory than inference due to:</p>
                    <ul class="list-disc list-inside space-y-3 text-gray-300 mb-6">
                        <li><strong class="text-white">Model weights:</strong> The parameters themselves</li>
                        <li><strong class="text-white">Gradients:</strong> Computed during backpropagation</li>
                        <li><strong class="text-white">Optimizer states:</strong> Adam optimizer stores momentum and variance</li>
                        <li><strong class="text-white">Activations:</strong> Intermediate values stored for gradient computation</li>
                    </ul>

                    <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                        <h4 class="font-bold text-xl mb-4 text-yellow-400">Memory Calculation Example</h4>
                        <p class="text-gray-300 mb-3">For a 7B parameter model using Adam optimizer in mixed precision (fp16):</p>
                        <div class="font-mono text-sm space-y-2 text-gray-400">
                            <p>• Model weights (fp16): 7B × 2 bytes = <span class="text-white">14 GB</span></p>
                            <p>• Gradients (fp16): 7B × 2 bytes = <span class="text-white">14 GB</span></p>
                            <p>• Optimizer states (fp32): 7B × 8 bytes = <span class="text-white">56 GB</span></p>
                            <p class="text-white font-bold">Total: ~84 GB (minimum)</p>
                        </div>
                        <p class="text-gray-400 mt-4 text-sm">Note: This excludes activation memory, which can be substantial</p>
                    </div>
                </div>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-4">Memory Optimization Strategies</h3>
                    <div class="space-y-4">
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-lg text-blue-400 mb-2">Gradient Checkpointing</h4>
                            <p class="text-gray-300">Trade computation for memory by recomputing activations during backpropagation instead of storing them all</p>
                        </div>
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-lg text-purple-400 mb-2">Mixed Precision Training</h4>
                            <p class="text-gray-300">Use fp16 for most operations while maintaining fp32 for critical computations</p>
                        </div>
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-lg text-green-400 mb-2">Parameter-Efficient Finetuning (PEFT)</h4>
                            <p class="text-gray-300">Only update a small subset of parameters, dramatically reducing memory requirements</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section 4: Techniques -->
        <section id="techniques" class="min-h-screen flex flex-col justify-center items-center px-8 py-24 border-b border-gray-800">
            <div class="max-w-5xl w-full">
                <h2 class="text-3xl font-bold text-white mb-12 text-center animated">Finetuning Techniques</h2>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 mb-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-6">Parameter-Efficient Finetuning (PEFT)</h3>
                    <p class="text-gray-300 mb-6">PEFT methods update only a small subset of parameters while keeping most of the pre-trained model frozen. This dramatically reduces memory requirements and training time.</p>

                    <div class="bg-indigo-900/20 border border-indigo-700 p-6 rounded-lg mb-6">
                        <h4 class="font-bold text-xl mb-4 text-indigo-300">LoRA (Low-Rank Adaptation)</h4>
                        <p class="text-gray-300 mb-4">LoRA is the most popular PEFT method. It works by decomposing weight updates into low-rank matrices:</p>
                        
                        <div class="bg-gray-900 p-6 rounded-lg mb-4">
                            <p class="text-gray-300 mb-2">Original weight update: $W' = W_0 + \Delta W$</p>
                            <p class="text-gray-300 mb-2">LoRA decomposition: $\Delta W = BA$</p>
                            <p class="text-gray-300 mb-4">Where $B \in \mathbb{R}^{d \times r}$ and $A \in \mathbb{R}^{r \times k}$ with rank $r \ll \min(d,k)$</p>
                            <p class="text-gray-400 text-sm">Typically, $r$ ranges from 1 to 64, resulting in massive parameter savings</p>
                        </div>

                        <ul class="list-disc list-inside space-y-2 text-gray-300">
                            <li><strong class="text-white">Memory Efficient:</strong> Only train and store small rank decomposition matrices</li>
                            <li><strong class="text-white">No Inference Latency:</strong> LoRA matrices can be merged back to original layers after training</li>
                            <li><strong class="text-white">Modular:</strong> Multiple LoRA adapters can be trained for different tasks</li>
                            <li><strong class="text-white">Performance:</strong> Often matches full finetuning quality with <1% of trainable parameters</li>
                        </ul>

                        <blockquote class="border-l-4 border-blue-500 pl-6 italic text-gray-400 mt-6">
                            <p>"The LoRA matrices can be merged back to the original layers after training, meaning there is no extra inference latency."</p>
                            <footer class="text-right mt-2 text-gray-500">- Huyen (2025, p. 342)</footer>
                        </blockquote>
                    </div>

                    <div class="grid md:grid-cols-2 gap-6">
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-lg text-purple-400 mb-3">Other PEFT Methods</h4>
                            <ul class="list-disc list-inside space-y-2 text-gray-300 text-sm">
                                <li><strong>Prefix Tuning:</strong> Add trainable prefix tokens</li>
                                <li><strong>Adapter Layers:</strong> Insert small trainable modules</li>
                                <li><strong>Prompt Tuning:</strong> Learn soft prompts</li>
                                <li><strong>IA3:</strong> Learned vector scaling</li>
                            </ul>
                        </div>
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-lg text-green-400 mb-3">Choosing PEFT Rank</h4>
                            <p class="text-gray-300 text-sm">Higher rank = more capacity but more parameters. Start small (r=8-16) and increase if needed. Most tasks work well with r=16-32.</p>
                        </div>
                    </div>
                </div>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-4">Advanced Techniques</h3>
                    <div class="space-y-6">
                        <div>
                            <h4 class="font-bold text-xl text-blue-400 mb-3">Model Merging</h4>
                            <p class="text-gray-300 mb-3">Combine multiple finetuned models to create a single model with multi-task capabilities. Different merging strategies include:</p>
                            <ul class="list-disc list-inside space-y-2 text-gray-300 ml-4">
                                <li><strong>Average merging:</strong> Simple parameter averaging</li>
                                <li><strong>TIES:</strong> Trim, elect, and merge based on parameter importance</li>
                                <li><strong>DARE:</strong> Drop and rescale to reduce interference</li>
                            </ul>
                        </div>
                        <div>
                            <h4 class="font-bold text-xl text-purple-400 mb-3">Continual Learning</h4>
                            <p class="text-gray-300">Techniques to learn new tasks sequentially without catastrophic forgetting of previous knowledge. Important for deploying models that need to adapt over time.</p>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Section 5: Practical Guide -->
        <section id="practical" class="min-h-screen flex flex-col justify-center items-center px-8 py-24 border-b border-gray-800">
            <div class="max-w-5xl w-full">
                <h2 class="text-3xl font-bold text-white mb-12 text-center animated">Practical Finetuning Guide</h2>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 mb-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-6">Finetuning Workflow</h3>
                    <div class="space-y-6">
                        <div class="flex items-start gap-4">
                            <div class="flex-shrink-0 w-10 h-10 bg-indigo-600 rounded-full flex items-center justify-center text-white font-bold">1</div>
                            <div>
                                <h4 class="font-bold text-lg text-white mb-2">Select Base Model</h4>
                                <p class="text-gray-300">Choose a pre-trained model appropriate for your task. Consider size, capabilities, and licensing.</p>
                            </div>
                        </div>
                        <div class="flex items-start gap-4">
                            <div class="flex-shrink-0 w-10 h-10 bg-indigo-600 rounded-full flex items-center justify-center text-white font-bold">2</div>
                            <div>
                                <h4 class="font-bold text-lg text-white mb-2">Prepare Training Data</h4>
                                <p class="text-gray-300">Collect and format high-quality examples. Quality matters more than quantity - aim for diverse, representative samples.</p>
                            </div>
                        </div>
                        <div class="flex items-start gap-4">
                            <div class="flex-shrink-0 w-10 h-10 bg-indigo-600 rounded-full flex items-center justify-center text-white font-bold">3</div>
                            <div>
                                <h4 class="font-bold text-lg text-white mb-2">Configure Finetuning</h4>
                                <p class="text-gray-300">Choose PEFT method (typically LoRA), set hyperparameters (rank, learning rate, batch size).</p>
                            </div>
                        </div>
                        <div class="flex items-start gap-4">
                            <div class="flex-shrink-0 w-10 h-10 bg-indigo-600 rounded-full flex items-center justify-center text-white font-bold">4</div>
                            <div>
                                <h4 class="font-bold text-lg text-white mb-2">Train and Monitor</h4>
                                <p class="text-gray-300">Run training with validation checks. Monitor for overfitting and adjust as needed.</p>
                            </div>
                        </div>
                        <div class="flex items-start gap-4">
                            <div class="flex-shrink-0 w-10 h-10 bg-indigo-600 rounded-full flex items-center justify-center text-white font-bold">5</div>
                            <div>
                                <h4 class="font-bold text-lg text-white mb-2">Evaluate and Iterate</h4>
                                <p class="text-gray-300">Test on held-out data, compare to baseline, and refine based on results.</p>
                            </div>
                        </div>
                    </div>
                </div>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 mb-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-6">Key Hyperparameters</h3>
                    <div class="grid md:grid-cols-2 gap-6">
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-lg text-blue-400 mb-3">Learning Rate</h4>
                            <p class="text-gray-300 mb-2">Typical range: 1e-5 to 5e-4</p>
                            <p class="text-gray-400 text-sm">Start conservative (1e-4) and adjust based on training dynamics</p>
                        </div>
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-lg text-purple-400 mb-3">LoRA Rank</h4>
                            <p class="text-gray-300 mb-2">Typical range: 8 to 64</p>
                            <p class="text-gray-400 text-sm">Higher rank = more capacity, more parameters. Start with 16.</p>
                        </div>
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-lg text-green-400 mb-3">Batch Size</h4>
                            <p class="text-gray-300 mb-2">As large as memory allows</p>
                            <p class="text-gray-400 text-sm">Use gradient accumulation if batch size is limited</p>
                        </div>
                        <div class="bg-gray-900 p-6 rounded-lg border border-gray-700">
                            <h4 class="font-bold text-lg text-yellow-400 mb-3">Epochs</h4>
                            <p class="text-gray-300 mb-2">Typically 3-5 epochs</p>
                            <p class="text-gray-400 text-sm">Monitor validation loss to avoid overfitting</p>
                        </div>
                    </div>
                </div>

                <div class="bg-gray-800 border border-gray-700 rounded-xl p-8 animated">
                    <h3 class="text-2xl font-semibold text-indigo-400 mb-6">Best Practices</h3>
                    <ul class="space-y-4 text-gray-300">
                        <li class="flex items-start gap-3">
                            <svg class="w-6 h-6 text-green-500 flex-shrink-0 mt-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg>
                            <span><strong class="text-white">Start with prompt engineering:</strong> Exhaust prompt-based methods before finetuning</span>
                        </li>
                        <li class="flex items-start gap-3">
                            <svg class="w-6 h-6 text-green-500 flex-shrink-0 mt-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg>
                            <span><strong class="text-white">Quality over quantity:</strong> Better to have 100 high-quality examples than 1000 noisy ones</span>
                        </li>
                        <li class="flex items-start gap-3">
                            <svg class="w-6 h-6 text-green-500 flex-shrink-0 mt-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg>
                            <span><strong class="text-white">Use PEFT by default:</strong> LoRA is effective for most use cases and much more efficient</span>
                        </li>
                        <li class="flex items-start gap-3">
                            <svg class="w-6 h-6 text-green-500 flex-shrink-0 mt-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg>
                            <span><strong class="text-white">Monitor for overfitting:</strong> Use validation sets and early stopping</span>
                        </li>
                        <li class="flex items-start gap-3">
                            <svg class="w-6 h-6 text-green-500 flex-shrink-0 mt-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg>
                            <span><strong class="text-white">Evaluate thoroughly:</strong> Test on diverse scenarios beyond training distribution</span>
                        </li>
                        <li class="flex items-start gap-3">
                            <svg class="w-6 h-6 text-green-500 flex-shrink-0 mt-1" fill="none" stroke="currentColor" viewBox="0 0 24 24"><path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M5 13l4 4L19 7"></path></svg>
                            <span><strong class="text-white">Version control:</strong> Track model checkpoints, data versions, and hyperparameters</span>
                        </li>
                    </ul>
                </div>
            </div>
        </section>

        <!-- Key Takeaways -->
        <section id="takeaways" class="min-h-screen flex flex-col justify-center items-center px-8 py-24 border-b border-gray-800">
            <div class="max-w-5xl w-full">
                <h2 class="text-3xl font-bold text-white mb-12 text-center animated">Key Takeaways</h2>
                <div class="grid md:grid-cols-2 gap-8">
                    <div class="card animated" style="transition-delay: 100ms;">
                        <p class="text-lg text-center">1. <span class="font-semibold text-indigo-400">Finetuning modifies weights:</span> Unlike prompting, it adapts the model by changing parameters</p>
                    </div>
                    <div class="card animated" style="transition-delay: 200ms;">
                        <p class="text-lg text-center">2. <span class="font-semibold text-indigo-400">Memory is the bottleneck:</span> Training requires much more memory than inference</p>
                    </div>
                    <div class="card animated" style="transition-delay: 300ms;">
                        <p class="text-lg text-center">3. <span class="font-semibold text-indigo-400">PEFT enables efficiency:</span> LoRA and similar methods make finetuning accessible</p>
                    </div>
                    <div class="card animated" style="transition-delay: 400ms;">
                        <p class="text-lg text-center">4. <span class="font-semibold text-indigo-400">Quality matters most:</span> Focus on high-quality training data over quantity</p>
                    </div>
                    <div class="card animated" style="transition-delay: 500ms;">
                        <p class="text-lg text-center">5. <span class="font-semibold text-indigo-400">Exhaust alternatives first:</span> Try prompting and RAG before finetuning</p>
                    </div>
                    <div class="card animated" style="transition-delay: 600ms;">
                        <p class="text-lg text-center">6. <span class="font-semibold text-indigo-400">Evaluate thoroughly:</span> Test finetuned models on diverse scenarios</p>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <!-- Footer -->
    <footer class="text-center py-12 text-gray-400 border-t border-gray-800">
        <p class="mb-8"><strong>Source:</strong> Huyen, C. (2025). <em>AI Engineering: Building Applications with Foundation Models</em>. O'Reilly Media.</p>
        <a href="index.html" class="inline-block bg-indigo-600 hover:bg-indigo-700 text-white font-semibold px-6 py-3 rounded-lg transition-colors">
            ← Back to All Lessons
        </a>
    </footer>

    <!-- Shared utilities -->
    <script src="/js/utils.js"></script>
</body>
</html>
