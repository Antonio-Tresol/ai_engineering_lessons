<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Chapter 3: Evaluation Methodology</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            scroll-behavior: smooth;
        }
        .module-card {
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }
        .module-card:hover {
            transform: translateY(-5px);
            box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
        }
        .interactive-section {
            border: 1px solid #e2e8f0;
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin-top: 1.5rem;
            background-color: #f8fafc;
        }
        .btn {
            padding: 0.5rem 1rem;
            border-radius: 0.5rem;
            font-weight: 500;
            transition: background-color 0.2s ease;
            cursor: pointer;
        }
        .btn-primary {
            background-color: #4f46e5;
            color: white;
        }
        .btn-primary:hover {
            background-color: #4338ca;
        }
        .btn-secondary {
            background-color: #e5e7eb;
            color: #374151;
        }
        .btn-secondary:hover {
            background-color: #d1d5db;
        }
        .feedback {
            padding: 0.75rem;
            border-radius: 0.5rem;
            margin-top: 1rem;
            font-size: 0.875rem;
        }
        .feedback-correct {
            background-color: #dcfce7;
            color: #166534;
            border: 1px solid #86efac;
        }
        .feedback-incorrect {
            background-color: #fee2e2;
            color: #991b1b;
            border: 1px solid #fca5a5;
        }
        .bias-btn.active {
            background-color: #4f46e5;
            color: white;
            border-color: #4f46e5;
        }
        .elo-change {
            font-size: 0.8rem;
            font-weight: bold;
        }
        .elo-change.positive {
            color: #16a34a;
        }
        .elo-change.negative {
            color: #dc2626;
        }
    </style>
</head>
<body class="bg-gray-50 text-gray-800">

    <!-- Header and Navigation -->
    <header class="bg-white shadow-sm sticky top-0 z-50">
        <nav class="container mx-auto px-6 py-4 flex justify-between items-center">
            <h1 class="text-xl font-bold text-gray-900">Evaluation Methodology Guide</h1>
            <div class="hidden md:flex space-x-4">
                <a href="#module1" class="text-gray-600 hover:text-indigo-600">Imperative</a>
                <a href="#module2" class="text-gray-600 hover:text-indigo-600">Metrics</a>
                <a href="#module3" class="text-gray-600 hover:text-indigo-600">AI Judge</a>
                <a href="#module4" class="text-gray-600 hover:text-indigo-600">Ranking</a>
                <a href="#module5" class="text-gray-600 hover:text-indigo-600">Synthesis</a>
            </div>
        </nav>
    </header>

    <main class="container mx-auto px-6 py-12">
        <div class="text-center mb-12">
            <h2 class="text-4xl font-extrabold text-gray-900">Chapter 3: Evaluation Methodology</h2>
            <p class="mt-4 text-lg text-gray-600">The challenge of selecting a model.</p>
        </div>

        <!-- Module 1: The Evaluation Imperative -->
        <section id="module1" class="mb-16">
            <div class="module-card bg-white p-8 rounded-xl shadow-lg">
                <h3 class="text-2xl font-bold mb-1 text-indigo-700">1. The Evaluation Imperative</h3>
                <p class="text-gray-500 mb-6">Why Evaluation is Critical & Its Unique Challenges for Foundation Models.</p>
                
                <h4 class="font-semibold text-lg mb-4">High-Stakes AI Failures</h4>
                <div class="grid md:grid-cols-2 gap-4 mb-6">
                    <div class="border border-red-200 bg-red-50 p-4 rounded-lg">
                        <h5 class="font-bold">Air Canada's Chatbot</h5>
                        <p class="text-sm">Gave incorrect bereavement fare information, forcing the airline to honor the mistake after a lawsuit.</p>
                    </div>
                    <div class="border border-red-200 bg-red-50 p-4 rounded-lg">
                        <h5 class="font-bold">Lawyers' Hallucinated Evidence</h5>
                        <p class="text-sm">Attorneys faced sanctions for submitting legal briefs with fake case citations generated by an LLM.</p>
                    </div>
                </div>
                <p class="italic text-gray-700">These examples show that robust evaluation is not academic but a critical business and safety need. As noted, "figuring out evaluation can take up the majority of the development effort."</p>

                <div class="interactive-section">
                    <h4 class="font-semibold text-lg mb-2">Activity: Think-Pair-Share Simulation</h4>
                    <p class="text-gray-600 mb-4">Consider an AI application you're working on. What is the biggest risk of evaluation failure? Identify one aspect of its performance that is difficult to measure with a simple number.</p>
                    <textarea id="tps-input" class="w-full p-2 border rounded-md h-24" placeholder="Type your thoughts here..."></textarea>
                    <button id="tps-reveal" class="btn btn-primary mt-3">Reveal Key Concepts</button>
                    <div id="tps-concepts" class="hidden mt-4 p-4 bg-indigo-50 border border-indigo-200 rounded-md">
                        <h5 class="font-bold">Potential areas to consider:</h5>
                        <ul class="list-disc list-inside text-sm text-gray-700 mt-2">
                            <li>**Nuanced User Satisfaction:** Is the user happy, even if the answer is technically correct but has the wrong tone?</li>
                            <li>**Factual Accuracy in Open-Ended Domains:** How do you verify facts when there's no single ground truth database? (e.g., historical analysis)</li>
                            <li>**Harmful Content Generation:** How do you measure a model's failure to AVOID generating subtle forms of bias or harmful advice?</li>
                            <li>**Long-Term Consequences:** How does the chatbot's advice impact a user's decisions over days or weeks?</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <!-- Module 2: Foundational & Exact Metrics -->
        <section id="module2" class="mb-16">
            <div class="module-card bg-white p-8 rounded-xl shadow-lg">
                <h3 class="text-2xl font-bold mb-1 text-indigo-700">2. Foundational & Exact Metrics</h3>
                <p class="text-gray-500 mb-6">From Language Modeling Metrics to Functional Correctness.</p>
                
                <div class="prose prose-indigo max-w-none mb-8 text-gray-700">
                    <p>This module covers the fundamentals of measurement, divided into two parts. The first part examines metrics for evaluating a core <em>language model's</em> understanding of language. The second part focuses on 'Exact Evaluation' metrics, which measure the quality of the final <em>output</em> for a specific task.</p>
                
                    <h4 class="text-xl font-bold mt-8 mb-4 text-gray-800">Part A: Language Modeling Metrics (The Model's Core)</h4>
                    <p>These deep, internal training metrics are important for two reasons. First, they serve as a powerful <strong>proxy for a model's overall capability</strong>. A model with a better fundamental understanding of language will likely perform better on downstream tasks. Second, they have practical applications in <strong>data validation and cleaning</strong>.</p>
                
                    <div class="mt-6">
                        <h5 class="text-lg font-semibold text-gray-800">1. Entropy: Measuring Uncertainty in Data</h5>
                        <ul class="list-disc list-inside mt-2 space-y-2">
                            <li><strong>Core Intuition:</strong> Entropy is not a measure of the model, but of the <strong>data itself</strong>. It quantifies the average amount of surprise or uncertainty inherent in the language.</li>
                            <li><strong>How It Works:</strong> It answers the question: "On average, how much new information does each new word (or token) give me?"</li>
                            <li><strong>Example:</strong> A language with two options ("upper", "lower") has low entropy (1 bit). A language with four options ("upper-left," etc.) has more uncertainty and higher entropy (2 bits).</li>
                        </ul>
                    </div>
                
                    <div class="mt-6">
                        <h5 class="text-lg font-semibold text-gray-800">2. Cross-Entropy: Measuring a Model's "Surprise"</h5>
                        <ul class="list-disc list-inside mt-2 space-y-2">
                            <li><strong>Core Intuition:</strong> If entropy measures the data's inherent uncertainty, cross-entropy measures <strong>how surprised our model is by that data</strong>. A lower cross-entropy means the model is less surprised, indicating it has learned the patterns in the data well.</li>
                            <li><strong>How It Works:</strong> It can be thought of as `Model's Cross Entropy = Data's Inherent Entropy + The Model's Error`. Training a model aims to minimize this error.</li>
                            <li><strong>Bits-per-Character (BPC) & Bits-per-Byte (BPB):</strong> These are simply <strong>normalized</strong> versions of cross-entropy, allowing for apples-to-apples comparisons between models with different tokenizers.</li>
                        </ul>
                    </div>
                    
                    <div class="mt-6">
                        <h5 class="text-lg font-semibold text-gray-800">3. Perplexity (PPL): The Engineer's Go-To Metric</h5>
                        <ul class="list-disc list-inside mt-2 space-y-2">
                            <li><strong>Core Intuition:</strong> The exponential of cross-entropy, it can be thought of as the <strong>effective number of choices the model is considering for the next token</strong>. A perplexity of 10 means the model is as uncertain as if it were choosing between 10 equally likely words.</li>
                            <li><strong>Practical Use Cases for Engineers:</strong>
                                <ul class="list-disc list-inside ml-5 mt-2 space-y-1">
                                    <li><strong>Proxy for Model Capability:</strong> Lower PPL generally correlates with a more powerful model.</li>
                                    <li><strong>Detecting Data Contamination:</strong> An extremely low PPL on a benchmark is a red flag that it was in the training data.</li>
                                    <li><strong>Detecting Abnormal Inputs:</strong> Gibberish or nonsensical text will have a very high PPL.</li>
                                </ul>
                            </li>
                            <li><strong>The Critical Caveat:</strong> Post-training techniques like instruction tuning (SFT) and RLHF often <strong>increase</strong> a model's perplexity. The model is being trained to be helpful, not just to predict the most statistically likely next token.</li>
                        </ul>
                    </div>
                
                    <hr class="my-8 border-gray-200">
                
                    <h4 class="text-xl font-bold mt-6 mb-4 text-gray-800">Part B: Exact Evaluation Metrics (The Application's Output)</h4>
                    <p>Exact evaluation provides a clear, unambiguous, and objective judgment of a model's output.</p>
                
                    <div class="mt-6">
                        <h5 class="text-lg font-semibold text-gray-800">1. Functional Correctness: The Gold Standard</h5>
                        <ul class="list-disc list-inside mt-2 space-y-2">
                            <li><strong>Core Intuition:</strong> This is the ultimate goal. The metric simply asks: <strong>"Did the AI system accomplish the task it was supposed to?"</strong></li>
                            <li><strong>When to Use:</strong> This is the preferred method whenever the output's correctness can be automatically and programmatically verified.</li>
                            <li><strong>Examples:</strong> Running unit tests on generated code (the `pass@k` metric) or executing generated SQL and checking if the returned result matches the expected result.</li>
                        </ul>
                    </div>
                
                    <div class="mt-6">
                        <h5 class="text-lg font-semibold text-gray-800">2. Similarity Measurements Against Reference Data: The Next Best Thing</h5>
                        <p class="my-2">For tasks like translation or summarization, where a single 'correct' output cannot be verified by a unit test, similarity metrics are used to compare the model's output against human-created examples.</p>
                        
                        <h6 class="font-semibold mt-4 text-gray-800">Lexical Similarity (e.g., BLEU, ROUGE)</h6>
                        <ul class="list-disc list-inside mt-2 space-y-2">
                            <li><strong>Core Intuition:</strong> An older approach that measures the <strong>literal, surface-level overlap of words and phrases</strong> (n-grams) between the generated text and a human-written reference text.</li>
                            <li><strong>Why It's Flawed:</strong> It's rigid, penalizes perfectly valid synonyms and paraphrasing, and is often a poor signal for true quality or correctness. OpenAI found that BLEU scores for incorrect and correct solutions were similar, proving it's not a reliable indicator.</li>
                        </ul>
                
                        <h6 class="font-semibold mt-4 text-gray-800">Semantic Similarity (Embedding-based)</h6>
                        <ul class="list-disc list-inside mt-2 space-y-2">
                            <li><strong>Core Intuition:</strong> This is the modern, superior approach. Instead of comparing words, <strong>it compares the underlying meaning</strong>.</li>
                            <li><strong>How It Works:</strong> Both the model's output and the reference text are fed into a specialized embedding model which converts each text into a vector. We then calculate the mathematical similarity (e.g., cosine similarity) between these two vectors.</li>
                            <li><strong>Advantage vs. Limitation:</strong> It is far more robust and flexible than lexical similarity. However, its reliability depends on the quality of the underlying embedding algorithm.</li>
                        </ul>
                    </div>
                </div>

                <hr class="my-8 border-t-2 border-indigo-100">

                <div class="interactive-section">
                    <h4 class="font-semibold text-lg mb-2">Interactive: Perplexity Simulator</h4>
                    <p class="text-gray-600 mb-4">Perplexity measures how "surprised" a model is by a piece of text. Lower is better. See how it changes based on model tuning.</p>
                    
                    <div class="flex flex-wrap gap-4 items-center">
                         <div>
                            <label for="perplexity-sentence" class="block text-sm font-medium text-gray-700">Choose a sentence:</label>
                            <select id="perplexity-sentence" class="mt-1 block w-full pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm rounded-md">
                                <option value="standard">The sun rises in the east.</option>
                                <option value="complex">Mechanistic interpretability seeks to reverse-engineer neural networks.</option>
                            </select>
                        </div>
                        <div>
                            <label for="perplexity-model" class="block text-sm font-medium text-gray-700">Choose a model type:</label>
                            <select id="perplexity-model" class="mt-1 block w-full pl-3 pr-10 py-2 text-base border-gray-300 focus:outline-none focus:ring-indigo-500 focus:border-indigo-500 sm:text-sm rounded-md">
                                <option value="good">Well-Trained Base Model</option>
                                <option value="bad">Poorly-Trained Model</option>
                                <option value="tuned">Instruction-Tuned Model</option>
                            </select>
                        </div>
                    </div>

                    <div id="perplexity-result" class="mt-6 p-4 bg-gray-100 rounded-md text-center">
                        <p class="text-lg">Perplexity Score: <span id="perplexity-score" class="font-bold text-2xl text-indigo-600">--</span></p>
                        <p id="perplexity-explanation" class="text-sm text-gray-600 mt-2"></p>
                    </div>
                </div>

                <div class="interactive-section">
                    <h4 class="font-semibold text-lg mb-2">Game: Match the Metric</h4>
                    <p class="text-gray-600 mb-4">Drag the use case to the correct evaluation metric.</p>
                    <div class="flex flex-col md:flex-row gap-8">
                        <div class="w-full md:w-1/2">
                            <h5 class="font-semibold mb-2">Metrics</h5>
                            <div id="metric-targets" class="space-y-3">
                                <div class="p-4 border rounded-lg bg-white" data-metric="correctness"><span class="font-bold">Functional Correctness</span><div class="drop-zone min-h-[40px] bg-gray-100 rounded-md mt-1 p-2"></div></div>
                                <div class="p-4 border rounded-lg bg-white" data-metric="lexical"><span class="font-bold">Lexical Similarity (BLEU/ROUGE)</span><div class="drop-zone min-h-[40px] bg-gray-100 rounded-md mt-1 p-2"></div></div>
                                <div class="p-4 border rounded-lg bg-white" data-metric="semantic"><span class="font-bold">Semantic Similarity</span><div class="drop-zone min-h-[40px] bg-gray-100 rounded-md mt-1 p-2"></div></div>
                            </div>
                        </div>
                        <div class="w-full md:w-1/2">
                            <h5 class="font-semibold mb-2">Use Cases</h5>
                            <div id="use-cases" class="space-y-2">
                                <div class="p-2 border rounded-md bg-indigo-100 cursor-grab" draggable="true" data-case="lexical">Comparing n-gram overlap for machine translation summaries.</div>
                                <div class="p-2 border rounded-md bg-indigo-100 cursor-grab" draggable="true" data-case="semantic">Checking if a paraphrased sentence retains its original meaning.</div>
                                <div class="p-2 border rounded-md bg-indigo-100 cursor-grab" draggable="true" data-case="correctness">Running unit tests on AI-generated code to see if it works.</div>
                            </div>
                        </div>
                    </div>
                     <div id="metric-feedback" class="feedback hidden"></div>
                </div>
            </div>
        </section>

        <!-- Module 3: The Subjective Judge: Using AI -->
        <section id="module3" class="mb-16">
            <div class="module-card bg-white p-8 rounded-xl shadow-lg">
                <h3 class="text-2xl font-bold mb-1 text-indigo-700">3. The Subjective Judge: Using AI</h3>
                <p class="text-gray-500 mb-6">Why, How, and its Limitations, Biases, and Specialized Judges.</p>
                <div class="interactive-section">
                    <h4 class="font-semibold text-lg mb-2">Interactive: AI Judge Bias Explorer</h4>
                    <p class="text-gray-600 mb-4">See how different biases can lead an AI Judge to the wrong conclusion. Select a bias and see how it evaluates the two responses below.</p>
                    <p class="mb-3"><span class="font-semibold">Prompt:</span> "Summarize the concept of photosynthesis in one sentence."</p>

                    <div class="grid md:grid-cols-2 gap-4 mb-4">
                        <div id="responseA" class="p-4 border-2 border-transparent rounded-lg bg-gray-100">
                           <h5 class="font-bold">Response A</h5>
                           <p>Photosynthesis is the process used by plants, algae, and certain bacteria to convert light energy into chemical energy, through a process that converts carbon dioxide and water into glucose and oxygen.</p>
                        </div>
                        <div id="responseB" class="p-4 border-2 border-transparent rounded-lg bg-gray-100">
                           <h5 class="font-bold">Response B</h5>
                           <p>Photosynthesis is how plants make food from sunlight.</p>
                        </div>
                    </div>
                    
                    <div class="mb-4">
                        <label class="block text-sm font-medium text-gray-700 mb-2">Select a Judge Bias:</label>
                        <div class="flex flex-wrap gap-2">
                            <button class="bias-btn btn btn-secondary" data-bias="none">Unbiased</button>
                            <button class="bias-btn btn btn-secondary" data-bias="verbosity">Verbosity Bias</button>
                            <button class="bias-btn btn btn-secondary" data-bias="position">Position Bias</button>
                            <button class="bias-btn btn btn-secondary" data-bias="self">Self-Bias (if judge was Model A)</button>
                        </div>
                    </div>
                    
                    <div id="bias-explanation" class="p-4 bg-indigo-50 border border-indigo-200 rounded-md">
                        <h5 class="font-bold">Judge's Verdict:</h5>
                        <p id="bias-verdict">Select a bias to see the evaluation.</p>
                        <p id="bias-reason" class="text-sm mt-2"></p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Module 4: The Competitive Arena: Ranking Models -->
        <section id="module4" class="mb-16">
            <div class="module-card bg-white p-8 rounded-xl shadow-lg">
                <h3 class="text-2xl font-bold mb-1 text-indigo-700">4. The Competitive Arena</h3>
                <p class="text-gray-500 mb-6">Ranking Models with Comparative Evaluation.</p>

                <div class="prose prose-indigo max-w-none mb-8 text-gray-700">
                    <p>Previous modules discussed metrics that provide a concrete score for a model's performance. However, a common business question is not "What is the score of Model A?" but rather, <strong>"Is Model A better than Model B for our use case?"</strong> This leads to the critical area of ranking models through comparative evaluation.</p>

                    <h4 class="text-xl font-bold mt-8 mb-4 text-gray-800">Part A: Pointwise vs. Comparative Evaluation</h4>
                    <p>The distinction between pointwise and comparative evaluation can be understood through an analogy of judging a figure skating competition.</p>
                    <ol class="list-decimal list-inside mt-2 space-y-2">
                        <li><strong>Pointwise Evaluation:</strong> In pointwise evaluation, each skater is scored individually on technical skill and artistry. The winner is the one with the highest total score. This is analogous to running a model against a benchmark like MMLU to get an accuracy score.</li>
                        <li><strong>Comparative Evaluation:</strong> In comparative evaluation, two skaters perform side-by-side, and a judge simply chooses which one they preferred. This forgoes a detailed, absolute score in favor of a simpler, relative judgment.</li>
                    </ol>
                    <p>For tasks where quality is subjective—like the creativity of a story or the helpfulness of a chatbot's response—humans find the second method much easier.</p>

                    <h5 class="text-lg font-semibold mt-6 text-gray-800">How It Works: The Chatbot Arena Model</h5>
                    <ul class="list-disc list-inside mt-2 space-y-2">
                        <li>This method was famously implemented by LMSYS in their Chatbot Arena. A user enters a prompt and gets responses from two anonymous models.</li>
                        <li>The user compares the two responses and votes for the better one. Only after voting are the model names revealed.</li>
                        <li>Each comparison is called a <strong>`match`</strong>. Over thousands of matches, this builds a massive dataset of pairwise preferences.</li>
                    </ul>

                    <h5 class="text-lg font-semibold mt-6 text-gray-800">The Ranking Problem: From Votes to a Leaderboard</h5>
                    <ul class="list-disc list-inside mt-2 space-y-2">
                        <li>How do you turn thousands of wins and losses into a single, ranked leaderboard? A simple win-loss record isn't enough because the quality of the opponent matters.</li>
                        <li><strong>The Solution:</strong> We borrow sophisticated rating algorithms from sports and gaming, like <strong>Elo</strong>, <strong>Bradley-Terry</strong>, or <strong>TrueSkill</strong>.</li>
                        <li>These systems update a model's rating based on the outcome of a match and the rating of its opponent. Beating a highly-rated model gives you a big rating boost, while losing to a low-rated model causes a big drop. The result is a single "Elo score" that allows for a ranked leaderboard.</li>
                    </ul>

                    <hr class="my-8 border-gray-200">

                    <h4 class="text-xl font-bold mt-6 mb-4 text-gray-800">Part B: The Hard Engineering Problems (Challenges)</h4>
                    
                    <h5 class="text-lg font-semibold mt-6 text-gray-800">1. Scalability & Data Intensity</h5>
                    <ul class="list-disc list-inside mt-2 space-y-2">
                        <li><strong>The `N²` Problem:</strong> The number of possible model pairs grows quadratically with the number of models. Evaluating 50 models means 1,225 pairs, which is incredibly data-intensive.</li>
                        <li><strong>The Transitivity Assumption:</strong> These algorithms assume that if A > B and B > C, then A > C. But human preference isn't always perfectly logical, which can introduce noise.</li>
                        <li><strong>Evaluating New & Private Models:</strong> Ranking a new model is costly, as it must be run against many others. Proprietary models require building an entire internal comparative framework.</li>
                    </ul>

                    <h5 class="text-lg font-semibold mt-6 text-gray-800">2. Lack of Standardization & Quality Control</h5>
                     <ul class="list-disc list-inside mt-2 space-y-2">
                        <li><strong>The "Garbage In, Garbage Out" Problem:</strong> While crowdsourcing captures real-world prompts, it has zero quality control.</li>
                        <li><strong>No Fact-Checking:</strong> Users might prefer a well-written but factually incorrect response.</li>
                        <li><strong>Polluted Data:</strong> The data can be polluted by very simple, low-effort prompts (e.g., "hello" or "hi") that don't differentiate models.</li>
                        <li><strong>No Grounding:</strong> Prompts often lack real-world context, like a RAG system where the model must use provided documents.</li>
                    </ul>

                    <h5 class="text-lg font-semibold mt-6 text-gray-800">3. The "So What?" Problem (From Comparative to Absolute Performance)</h5>
                    <ul class="list-disc list-inside mt-2 space-y-2">
                        <li>This is the most important challenge from a business perspective. You find that Model B beats Model A with a 51% win rate. Now what?</li>
                        <li><strong>The Ambiguity:</strong> That 51% win rate doesn't tell you the <em>magnitude</em> of the improvement. It could mean Model B is slightly better than an excellent Model A, or it could mean Model B is slightly less terrible than a bad Model A.</li>
                        <li><strong>The Business Decision:</strong> If Model B costs twice as much, this evaluation isn't sufficient to determine if the performance boost is worth the cost. It tells you the direction of improvement, but not the distance.</li>
                    </ul>
                    
                    <hr class="my-8 border-gray-200">

                    <h4 class="text-xl font-bold mt-6 mb-4 text-gray-800">Part C: The Future and Its Place in Your Toolkit</h4>
                    <p>Despite the challenges, this method is powerful when used correctly and is a durable component of evaluation strategy.</p>
                     <ul class="list-disc list-inside mt-2 space-y-2">
                        <li><strong>It Captures Human Preference:</strong> Ultimately, this is what we care about for many applications.</li>
                        <li><strong>It Doesn't Saturate:</strong> Unlike a fixed benchmark, you can always compare new and better models.</li>
                        <li><strong>It's Harder to "Game":</strong> It's much harder to "train for" the unpredictable nature of millions of human preferences than it is to train on a benchmark's test set.</li>
                        <li><strong>Key Takeaway:</strong> Comparative evaluation is not a replacement for exact metrics but rather a <strong>powerful complement</strong>. Benchmarks should be used for objective capabilities, while comparative evaluation helps in understanding subjective user preference.</li>
                    </ul>

                </div>

                <hr class="my-8 border-t-2 border-indigo-100">

                <div class="interactive-section">
                    <h4 class="font-semibold text-lg mb-2">Interactive: Elo Rating Simulator</h4>
                    <p class="text-gray-600 mb-4">Platforms like Chatbot Arena use the Elo rating system from chess to rank models based on head-to-head battles. You are the judge. Pick a winner and see how the Elo ratings change!</p>
                    
                    <div class="flex flex-col md:flex-row gap-8">
                        <!-- Leaderboard -->
                        <div class="w-full md:w-1/3">
                            <h5 class="font-semibold mb-3">Leaderboard</h5>
                            <div id="elo-leaderboard" class="space-y-2">
                                <!-- Models will be injected here by JS -->
                            </div>
                        </div>

                        <!-- Matchup -->
                        <div class="w-full md:w-2/3">
                            <h5 class="font-semibold mb-3">Current Matchup</h5>
                            <div id="elo-matchup" class="p-4 bg-gray-100 rounded-lg text-center">
                                <!-- Matchup will be injected here by JS -->
                            </div>
                             <div id="elo-feedback" class="feedback hidden"></div>
                        </div>
                    </div>
                </div>
            </div>
        </section>
        
        <!-- Module 5: Synthesis & Wrap-Up -->
        <section id="module5" class="mb-16">
            <div class="module-card bg-white p-8 rounded-xl shadow-lg">
                <h3 class="text-2xl font-bold mb-1 text-indigo-700">5. Synthesis & Wrap-Up</h3>
                <p class="text-gray-500 mb-6">Building a Robust, Multi-faceted Evaluation Pipeline.</p>

                <div class="interactive-section">
                    <h4 class="font-semibold text-lg mb-2">Activity: Evaluation Pipeline Builder</h4>
                    <p class="text-gray-600 mb-4">A robust strategy is a blend of techniques. For the scenario below, drag the evaluation methods into the pipeline stages based on their priority and function.</p>
                    <p class="mb-4 p-3 bg-indigo-50 rounded-md"><span class="font-semibold">Scenario:</span> You're building a text-to-SQL chatbot for your company's sales database. It must be highly accurate, produce efficient queries, and be trusted by non-technical users.</p>
                    
                    <div class="flex flex-col md:flex-row gap-8">
                        <!-- Pipeline Stages -->
                        <div class="w-full md:w-2/3">
                             <h5 class="font-semibold mb-2">Pipeline Stages</h5>
                            <div id="pipeline-targets" class="space-y-3">
                                <div class="p-4 border rounded-lg bg-white" data-stage="1"><span class="font-bold">Stage 1: Core Functionality (Non-negotiable)</span><div class="pipeline-drop-zone min-h-[40px] bg-gray-100 rounded-md mt-1 p-2"></div></div>
                                <div class="p-4 border rounded-lg bg-white" data-stage="2"><span class="font-bold">Stage 2: Scalable Quality & Preference Check</span><div class="pipeline-drop-zone min-h-[40px] bg-gray-100 rounded-md mt-1 p-2"></div></div>
                                <div class="p-4 border rounded-lg bg-white" data-stage="3"><span class="font-bold">Stage 3: Gold-Standard & Sanity Check</span><div class="pipeline-drop-zone min-h-[40px] bg-gray-100 rounded-md mt-1 p-2"></div></div>
                            </div>
                        </div>

                        <!-- Methods -->
                        <div class="w-full md:w-1/3">
                            <h5 class="font-semibold mb-2">Evaluation Methods</h5>
                             <div id="pipeline-methods" class="space-y-2">
                                <div class="p-2 border rounded-md bg-indigo-100 cursor-grab" draggable="true" data-method="exact"><strong>Exact Method:</strong> Execute generated SQL and compare results against ground truth.</div>
                                <div class="p-2 border rounded-md bg-indigo-100 cursor-grab" draggable="true" data-method="ai"><strong>AI as a Judge:</strong> Assess query efficiency, style, and readability at scale.</div>
                                <div class="p-2 border rounded-md bg-indigo-100 cursor-grab" draggable="true" data-method="human"><strong>Human Evaluation:</strong> Have expert data analysts review a subset of complex queries for logical errors and best practices.</div>
                             </div>
                        </div>
                    </div>
                     <button id="pipeline-check" class="btn btn-primary mt-4">Check My Pipeline</button>
                     <div id="pipeline-feedback" class="feedback hidden"></div>
                </div>
            </div>
        </section>

    </main>

    <footer class="bg-white border-t mt-12">
        <div class="container mx-auto px-6 py-4 text-center text-gray-500">
            <p class="text-sm">Source: Chip Huyen, <em>AI Engineering: Building Applications with Foundation Models</em>, Chapter 3 Evaluation Methodology 2025.</p>
        </div>
    </footer>

<script>
document.addEventListener('DOMContentLoaded', () => {

    // --- Module 1: Think-Pair-Share ---
    const tpsRevealBtn = document.getElementById('tps-reveal');
    const tpsConcepts = document.getElementById('tps-concepts');
    tpsRevealBtn.addEventListener('click', () => {
        tpsConcepts.classList.toggle('hidden');
    });

    // --- Module 2: Perplexity Simulator ---
    const perplexitySentence = document.getElementById('perplexity-sentence');
    const perplexityModel = document.getElementById('perplexity-model');
    const perplexityScore = document.getElementById('perplexity-score');
    const perplexityExplanation = document.getElementById('perplexity-explanation');

    function updatePerplexity() {
        const sentence = perplexitySentence.value;
        const model = perplexityModel.value;
        let score = 0;
        let explanation = '';

        const baseScores = {
            standard: { good: 15, bad: 150, tuned: 25 },
            complex: { good: 40, bad: 300, tuned: 55 }
        };

        score = baseScores[sentence][model];

        const explanations = {
            good: "A well-trained base model is not surprised by common or domain-specific language, resulting in a low perplexity score.",
            bad: "A poorly-trained model finds almost all text surprising, leading to a very high perplexity score. It has a poor grasp of the language.",
            tuned: "Instruction tuning makes a model better at tasks, but slightly worse at pure text prediction. Its goal shifts from predicting the next word to being helpful, so perplexity often increases slightly compared to the base model. This is a key distinction!"
        };
        
        explanation = explanations[model];
        
        // Add a bit of randomness for effect
        score = Math.round(score + (Math.random() - 0.5) * (score * 0.1));

        perplexityScore.textContent = score;
        perplexityExplanation.textContent = explanation;
    }

    perplexitySentence.addEventListener('change', updatePerplexity);
    perplexityModel.addEventListener('change', updatePerplexity);
    updatePerplexity();

    // --- Module 2: Metric Match-Up Game ---
    const useCases = document.querySelectorAll('#use-cases [draggable="true"]');
    const dropZones = document.querySelectorAll('.drop-zone');
    const metricFeedback = document.getElementById('metric-feedback');
    let draggedItem = null;

    useCases.forEach(item => {
        item.addEventListener('dragstart', (e) => {
            draggedItem = e.target;
            setTimeout(() => e.target.style.display = 'none', 0);
        });

        item.addEventListener('dragend', (e) => {
            setTimeout(() => {
                draggedItem.style.display = 'block';
                draggedItem = null;
            }, 0);
        });
    });

    dropZones.forEach(zone => {
        zone.addEventListener('dragover', (e) => {
            e.preventDefault();
            zone.classList.add('bg-indigo-100');
        });

        zone.addEventListener('dragleave', (e) => {
            zone.classList.remove('bg-indigo-100');
        });

        zone.addEventListener('drop', (e) => {
            e.preventDefault();
            zone.classList.remove('bg-indigo-100');
            if (zone.children.length === 0) { // Allow only one item per zone
                zone.appendChild(draggedItem);
                checkMetricMatches();
            }
        });
    });

    function checkMetricMatches() {
        const totalTargets = 3;
        let correctMatches = 0;
        
        document.querySelectorAll('#metric-targets > div').forEach(target => {
            const metric = target.getAttribute('data-metric');
            const dropZone = target.querySelector('.drop-zone');
            if (dropZone.children.length > 0) {
                const droppedCase = dropZone.children[0].getAttribute('data-case');
                if (metric === droppedCase) {
                    correctMatches++;
                }
            }
        });

        const allDropped = document.querySelectorAll('.drop-zone:empty').length === 0;

        if (allDropped) {
            metricFeedback.classList.remove('hidden');
            if (correctMatches === totalTargets) {
                metricFeedback.textContent = 'Excellent! You matched all metrics correctly.';
                metricFeedback.className = 'feedback feedback-correct';
            } else {
                metricFeedback.textContent = `You got ${correctMatches} out of ${totalTargets} correct. Review the concepts and try rearranging them.`;
                metricFeedback.className = 'feedback feedback-incorrect';
            }
        }
    }


    // --- Module 3: AI Judge Bias Explorer ---
    const biasBtns = document.querySelectorAll('.bias-btn');
    const biasVerdict = document.getElementById('bias-verdict');
    const biasReason = document.getElementById('bias-reason');
    const responseA = document.getElementById('responseA');
    const responseB = document.getElementById('responseB');

    biasBtns.forEach(btn => {
        btn.addEventListener('click', () => {
            biasBtns.forEach(b => b.classList.remove('active'));
            btn.classList.add('active');
            
            const bias = btn.getAttribute('data-bias');
            let verdict = '';
            let reason = '';
            
            responseA.style.borderColor = 'transparent';
            responseB.style.borderColor = 'transparent';

            switch(bias) {
                case 'none':
                    verdict = 'Response B is better.';
                    reason = 'The prompt asked for a one-sentence summary. Response B is concise and accurate, directly fulfilling the request. Response A is also correct but overly detailed for a single-sentence summary.';
                    responseB.style.borderColor = '#22c55e'; // green
                    break;
                case 'verbosity':
                    verdict = 'Response A is better.';
                    reason = 'This judge has a verbosity bias. It prefers longer, more detailed answers, even if a shorter answer is more appropriate for the prompt. It incorrectly rewards length over conciseness.';
                    responseA.style.borderColor = '#ef4444'; // red
                    break;
                case 'position':
                    verdict = 'Response A is better.';
                    reason = 'This judge has a position bias. It tends to favor the first option it sees, regardless of quality. This is a common artifact in comparative evaluations.';
                    responseA.style.borderColor = '#ef4444'; // red
                    break;
                case 'self':
                    verdict = 'Response A is better.';
                    reason = 'This judge has a self-bias. It recognizes the style of its own output (Response A) and prefers it over others. Models often rate their own generations more highly.';
                    responseA.style.borderColor = '#ef4444'; // red
                    break;
            }
            biasVerdict.textContent = verdict;
            biasReason.textContent = reason;
        });
    });
    // Set initial state
    document.querySelector('.bias-btn[data-bias="none"]').click();

    // --- Module 4: Elo Rating Simulator ---
    const leaderboardDiv = document.getElementById('elo-leaderboard');
    const matchupDiv = document.getElementById('elo-matchup');
    const eloFeedback = document.getElementById('elo-feedback');

    let models = [
        { name: 'Aqua-4T', rating: 1200, history: [1200] },
        { name: 'Blaze-2X', rating: 1150, history: [1150] },
        { name: 'Terra-1P', rating: 1100, history: [1100] },
        { name: 'Zephyr-7', rating: 1050, history: [1050] }
    ];

    let currentMatch = [];
    const K = 32; // K-factor for Elo calculation

    function updateEloLeaderboard() {
        models.sort((a, b) => b.rating - a.rating);
        leaderboardDiv.innerHTML = '';
        models.forEach(model => {
            const lastRating = model.history[model.history.length - 2] || model.rating;
            const change = model.rating - lastRating;
            
            let changeHtml = '';
            if (change > 0) {
                changeHtml = `<span class="elo-change positive">(+${change})</span>`;
            } else if (change < 0) {
                changeHtml = `<span class="elo-change negative">(${change})</span>`;
            }

            leaderboardDiv.innerHTML += `
                <div class="p-3 bg-white border rounded-lg flex justify-between items-center">
                    <span class="font-medium">${model.name}</span>
                    <span class="font-bold text-indigo-600">${model.rating} ${changeHtml}</span>
                </div>
            `;
        });
    }

    function pickNewMatchup() {
        let model1_idx, model2_idx;
        do {
            model1_idx = Math.floor(Math.random() * models.length);
            model2_idx = Math.floor(Math.random() * models.length);
        } while (model1_idx === model2_idx);
        
        currentMatch = [models[model1_idx], models[model2_idx]];
        
        matchupDiv.innerHTML = `
            <p class="text-xl mb-4"><span class="font-bold">${currentMatch[0].name}</span> vs <span class="font-bold">${currentMatch[1].name}</span></p>
            <p class="mb-4">Which model gave a better response?</p>
            <div class="flex justify-center gap-4">
                <button class="btn btn-primary w-1/2" onclick="resolveMatch(0)">${currentMatch[0].name} Wins</button>
                <button class="btn btn-primary w-1/2" onclick="resolveMatch(1)">${currentMatch[1].name} Wins</button>
            </div>
        `;
    }

    window.resolveMatch = function(winnerIndex) {
        const winner = currentMatch[winnerIndex];
        const loser = currentMatch[winnerIndex === 0 ? 1 : 0];

        const expectedWinner = 1 / (1 + Math.pow(10, (loser.rating - winner.rating) / 400));
        const expectedLoser = 1 / (1 + Math.pow(10, (winner.rating - loser.rating) / 400));
        
        const winnerNewRating = Math.round(winner.rating + K * (1 - expectedWinner));
        const loserNewRating = Math.round(loser.rating + K * (0 - expectedLoser));
        
        const winnerChange = winnerNewRating - winner.rating;
        const loserChange = loserNewRating - loser.rating;

        winner.rating = winnerNewRating;
        winner.history.push(winner.rating);
        loser.rating = loserNewRating;
        loser.history.push(loser.rating);
        
        eloFeedback.classList.remove('hidden');
        eloFeedback.className = 'feedback feedback-correct';
        eloFeedback.innerHTML = `
        <strong>Calculation Complete:</strong><br>
        ${winner.name}: ${winner.rating - winnerChange} -> <strong>${winner.rating}</strong> (+${winnerChange})<br>
        ${loser.name}: ${loser.rating - loserChange} -> <strong>${loser.rating}</strong> (${loserChange})
        `;

        updateEloLeaderboard();
        pickNewMatchup();
    }
    
    updateEloLeaderboard();
    pickNewMatchup();

     // --- Module 5: Pipeline Builder ---
    const pipelineMethods = document.querySelectorAll('#pipeline-methods [draggable="true"]');
    const pipelineDropZones = document.querySelectorAll('.pipeline-drop-zone');
    const pipelineCheckBtn = document.getElementById('pipeline-check');
    const pipelineFeedback = document.getElementById('pipeline-feedback');
    let draggedPipelineItem = null;

    pipelineMethods.forEach(item => {
        item.addEventListener('dragstart', (e) => {
            draggedPipelineItem = e.target;
            setTimeout(() => e.target.style.display = 'none', 0);
        });

        item.addEventListener('dragend', (e) => {
            setTimeout(() => {
                draggedPipelineItem.style.display = 'block';
                draggedPipelineItem = null;
            }, 0);
        });
    });

    pipelineDropZones.forEach(zone => {
        zone.addEventListener('dragover', (e) => e.preventDefault());
        zone.addEventListener('drop', (e) => {
            e.preventDefault();
            if(zone.children.length < 2) { // Allow multiple items
                zone.appendChild(draggedPipelineItem);
            }
        });
    });

    pipelineCheckBtn.addEventListener('click', () => {
        const stage1 = document.querySelector('[data-stage="1"] .pipeline-drop-zone');
        const stage2 = document.querySelector('[data-stage="2"] .pipeline-drop-zone');
        const stage3 = document.querySelector('[data-stage="3"] .pipeline-drop-zone');

        const stage1Method = stage1.children.length > 0 ? stage1.children[0].dataset.method : '';
        const stage2Method = stage2.children.length > 0 ? stage2.children[0].dataset.method : '';
        const stage3Method = stage3.children.length > 0 ? stage3.children[0].dataset.method : '';

        pipelineFeedback.classList.remove('hidden');
        if (stage1Method === 'exact' && stage2Method === 'ai' && stage3Method === 'human') {
            pipelineFeedback.className = 'feedback feedback-correct';
            pipelineFeedback.innerHTML = '<strong>Perfect!</strong> This is an ideal pipeline. You start with non-negotiable correctness, then use scalable AI for quality checks, and finally use human experts for the most nuanced gold-standard validation.';
        } else {
            pipelineFeedback.className = 'feedback feedback-incorrect';
            pipelineFeedback.innerHTML = '<strong>Not quite right.</strong> A good pipeline for text-to-SQL should prioritize functional correctness first (does the SQL run and give the right result?). Then, use scalable methods like AI Judges. Finally, use expensive but essential human evaluation as a final check. Please try again!';
        }
    });

});
</script>
</body>
</html>







